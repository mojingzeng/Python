{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy 框架课程介绍：\n",
    "   - 1.框架的简介和基础使用\n",
    "   - 2.持久化存储\n",
    "   - 3.日志和 cookie\n",
    "   - 4.CrawlSpider\n",
    "   - 5.基于 redis 的分布式爬虫\n",
    "\n",
    "#### 一、scrapy 框架的简介和基础使用\n",
    "   - 概念：为了爬取网站数据而编写的一款我强大的应用框架；\n",
    "         框架其实就是一个集成了相应的功能且具有很强通用性的项目模板。\n",
    "      - 特点：高性能的异步下载，解析，持久化\n",
    "   \n",
    "   - 安装： \n",
    "      - linux & mac os: pip install scrapy\n",
    "      - windows:\n",
    "         - pip insrall scrapy\n",
    "         - 下载 twisted: https://www.lfd.uci.edu/～gohlke/pythonlibs/#twisted\n",
    "              pip install xxx.whl\n",
    "         - pip install pywin32\n",
    "         - pip install scrapy\n",
    "        \n",
    "   - 基础使用：使用流程\n",
    "      - 创建一个工程：scrapy startproject proName\n",
    "      - 在工程目录:\n",
    "          \n",
    "         - scrapy.cfg      项目的主配置信息。（真正爬虫相关的配置信息在settings.py文件中\n",
    "         - items.py       设置数据存储模板，用于结构化数据\n",
    "         - pipelines.py     数据持久化处理\n",
    "         - settings.py     配置文件。如递归的层数，并发数，延迟下载\n",
    "         - spiders        爬虫目录，创建文件，编写爬虫解析规则\n",
    "         \n",
    "      - 在工程目录下创建一个爬虫文件：\n",
    "         - cd proName\n",
    "         - scrapy genspider first www.qiushibaike.com (起始url）\n",
    "      - 更改settings.py 文件： \n",
    "         - ROBOTSTXT_OBEF = False\n",
    "         - USER_AGENT =\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\"\n",
    "         - 执行： scrapy crawl first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy \n",
    "\n",
    "\n",
    "class FirstSpider(scrapy.spider):\n",
    "    # 爬虫文件的名称：通过爬虫文件的名称可以指定的定位符\n",
    "    name = 'first'\n",
    "    # 允许的域名： 只可以爬取指定域名下的页面数据\n",
    "    allowed_domains = ['www.qiushibaike.com']\n",
    "    # 起始url ：当前工程将要爬取的页面所对应的url\n",
    "    start_urls= ['http://www.qiushibaike.com']\n",
    "    \n",
    "    # 解析方法：对获取的页面数据进行指定内容的解析\n",
    "    # response：根据起始 url 列表发起请求，请求成功后返回的是响应对象\n",
    "    # parse 方法的返回值：必须为迭代器或者空\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # 获取响应对象中的页面数据\n",
    "        print(responds.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 发起 post 请求：\n",
    "   - 对 start_requests 方法进行重写。\n",
    "      - Request() 方法中给method属性赋值成 post\n",
    "      - FormRequest() 进行 post 请求的发送\n",
    "   - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cookie操作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代理：\n",
    "   - 下载中间件的作用：拦截请求，可以将请求的ip进行更换。\n",
    "   流程：\n",
    "      - 下载中间件类的自定义：\n",
    "         - 继承父类 object\n",
    "         - 重写 process_request(self,request,spider)的方法\n",
    "      - 配置文件中进行下载中间件的开启"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 日志等级：\n",
    "   - ERROR: 错误\n",
    "   - WARNING: 警告\n",
    "   - INFO: 一般信息\n",
    "   - DEBUG: 调试信息(默认)\n",
    "   - 自定义：指定终端输出指定种类的日志信息\n",
    "      - 在settings.py中增加一行代码\n",
    "         - LOG_LEVEL = 'ERROR'  # 指定的日志等级\n",
    "         - LOG_FILE = 'log.txt' # 指定log文件输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 请求传参：\n",
    "   -  \n",
    "   -\n",
    "   -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### crawlSpider\n",
    "   - 是spider的一个子类；\n",
    "   - 链接提取器，规则解析器\n",
    "   - 用法：scrapy genspider -t crawl spiderName address\n",
    "   - LinkExtractor(allow=r'Items/')   # 链接提取器，用来提取指定的链接\n",
    "      - 参数：allow = r'',  # 此处用正则表达式，用来提取链接\n",
    "   - follow=True: 表示在首次获取到页面的链接后继续往下获取页面链接\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
